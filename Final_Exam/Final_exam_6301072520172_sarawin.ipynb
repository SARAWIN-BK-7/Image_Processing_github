{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center\">Final exam Image Processing </h2>\n",
    "<h3 style=\"text-align: center\">Sarawin Buakaew ID: 63-010725-2017-2</h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from cv2 import aruco\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\">\n",
    "    Camera Calibration with OpenCV\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chessboard_size = (8,6) # Your chessboard size\n",
    "\n",
    "# iterative termination criteria, maximum iterationm and epsilon\n",
    "term_criteria = (cv.TermCriteria_EPS+ cv.TermCriteria_MAX_ITER, 30, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store object points and images point from all the images.\n",
    "obj_points = list() # 3D points in heterogeneous Xi\n",
    "img_points = list() # 2D points on image xi\n",
    "\n",
    "# Defining the world coordinates for 3D points Xi\n",
    "objp = np.zeros((1, chessboard_size[0] * chessboard_size[1], 3), np.float32)\n",
    "objp[0,:,:2] = np.mgrid[0:chessboard_size[0], 0:chessboard_size[1]].T.reshape(-1, 2)\n",
    "prev_img_shape = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(img) :\n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Term1_65\\Image_Processing\\Image_Processing_github\\Final_Exam/images/stereo/calibrationdata/\n",
      "left-0000.png\n",
      "left-0001.png\n",
      "left-0002.png\n",
      "left-0003.png\n",
      "left-0004.png\n",
      "left-0005.png\n",
      "left-0006.png\n",
      "left-0007.png\n",
      "left-0008.png\n",
      "left-0009.png\n",
      "left-0010.png\n",
      "left-0011.png\n",
      "left-0012.png\n",
      "left-0013.png\n",
      "left-0014.png\n",
      "left-0015.png\n",
      "left-0016.png\n",
      "left-0017.png\n",
      "left-0018.png\n",
      "left-0019.png\n",
      "left-0020.png\n",
      "left-0021.png\n",
      "left-0022.png\n",
      "left-0023.png\n",
      "left-0024.png\n",
      "left-0025.png\n",
      "left-0026.png\n",
      "left-0027.png\n",
      "left-0028.png\n",
      "left-0029.png\n",
      "left-0030.png\n",
      "left-0031.png\n",
      "left-0032.png\n",
      "left-0033.png\n",
      "left-0034.png\n",
      "left-0035.png\n",
      "left-0036.png\n",
      "left-0037.png\n",
      "left-0038.png\n",
      "left-0039.png\n",
      "left-0040.png\n",
      "left-0041.png\n",
      "left-0042.png\n",
      "left-0043.png\n",
      "left-0044.png\n",
      "left-0045.png\n",
      "left-0046.png\n",
      "left-0047.png\n",
      "left-0048.png\n",
      "left-0049.png\n",
      "left-0050.png\n",
      "left-0051.png\n",
      "left-0052.png\n",
      "left-0053.png\n",
      "left-0054.png\n",
      "left-0055.png\n",
      "left-0056.png\n",
      "right-0000.png\n",
      "right-0001.png\n",
      "right-0002.png\n",
      "right-0003.png\n",
      "right-0004.png\n",
      "right-0005.png\n",
      "right-0006.png\n",
      "right-0007.png\n",
      "right-0008.png\n",
      "right-0009.png\n",
      "right-0010.png\n",
      "right-0011.png\n",
      "right-0012.png\n",
      "right-0013.png\n",
      "right-0014.png\n",
      "right-0015.png\n",
      "right-0016.png\n",
      "right-0017.png\n",
      "right-0018.png\n",
      "right-0019.png\n",
      "right-0020.png\n",
      "right-0021.png\n",
      "right-0022.png\n",
      "right-0023.png\n",
      "right-0024.png\n",
      "right-0025.png\n",
      "right-0026.png\n",
      "right-0027.png\n",
      "right-0028.png\n",
      "right-0029.png\n",
      "right-0030.png\n",
      "right-0031.png\n",
      "right-0032.png\n",
      "right-0033.png\n",
      "right-0034.png\n",
      "right-0035.png\n",
      "right-0036.png\n",
      "right-0037.png\n",
      "right-0038.png\n",
      "right-0039.png\n",
      "right-0040.png\n",
      "right-0041.png\n",
      "right-0042.png\n",
      "right-0043.png\n",
      "right-0044.png\n",
      "right-0045.png\n",
      "right-0046.png\n",
      "right-0047.png\n",
      "right-0048.png\n",
      "right-0049.png\n",
      "right-0050.png\n",
      "right-0051.png\n",
      "right-0052.png\n",
      "right-0053.png\n",
      "right-0054.png\n",
      "right-0055.png\n",
      "right-0056.png\n"
     ]
    }
   ],
   "source": [
    "image_path = os.getcwd()+'/images/stereo/calibrationdata/'\n",
    "images = list()\n",
    "image_size = None\n",
    "print(image_path)\n",
    "\n",
    "# Preparing calibration files\n",
    "for filename in os.listdir(image_path) :\n",
    "    print(filename)\n",
    "    img = cv.imread(image_path+'{}'.format(filename))\n",
    "    images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images :\n",
    "    img_gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    ret, corners = cv.findChessboardCorners(img_gray, chessboard_size, None) # xi in pixel (20,15)\n",
    "    image_size = img_gray.shape\n",
    "    # If found add obj points, image points afterthat refining them\n",
    "    if ret == True :\n",
    "        #print('Found')\n",
    "        obj_points.append(objp) # Add Xi 3D\n",
    "        \n",
    "        corners2 = cv.cornerSubPix(img_gray, corners, (11,11), (-1,-1), term_criteria) # Refining xi -> xi in subpixel, xi -> floating point (19.7, 15.1)\n",
    "        img_points.append(corners2) # Add xi 2D\n",
    "\n",
    "        #Draw and display the chessboard corners\n",
    "        img = cv.drawChessboardCorners(image, chessboard_size, corners2, ret)\n",
    "        cv.imshow('frame', img)\n",
    "        cv.waitKey(100)\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center\">\n",
    "    <a href = \"https://docs.opencv.org/4.5.3/d9/d0c/group__calib3d.html#ga3207604e4b1a1758aa66acb6ed5aa65d\"> calibrateCamera </a>\n",
    "    is the main function which you can use to calibrate a pinhole camera model <br>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration\n"
     ]
    }
   ],
   "source": [
    "# # Start calibration argmin ||xi - PXi||^2 , xi = PXi\n",
    "print('Calibration')\n",
    "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(obj_points,img_points, image_size[::-1] , None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center\">\n",
    "    <a href = \"https://docs.opencv.org/4.5.3/d9/d0c/group__calib3d.html#ga7a6c4e032c97f03ba747966e6ad862b1\"> getOptimalNewCameraMatrix </a>\n",
    "    The function computes and returns the optimal new camera intrinsic matrix based on the free scaling parameter <br>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[726.47630219   0.         466.25758541]\n",
      " [  0.         732.0640605  251.997867  ]\n",
      " [  0.           0.           1.        ]]\n",
      "Optimal K\n",
      "[[728.84155273   0.         473.45067169]\n",
      " [  0.         728.33746338 251.73843936]\n",
      " [  0.           0.           1.        ]] (1, 6, 861, 468)\n"
     ]
    }
   ],
   "source": [
    "img = images[0].copy()\n",
    "h, w = img.shape[:2]\n",
    "newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx,dist,(w,h),1,(w,h))\n",
    "print(mtx)\n",
    "print('Optimal K')\n",
    "print(newcameramtx, roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center\">\n",
    "    <a href = \"https://docs.opencv.org/4.5.3/d9/d0c/group__calib3d.html#ga69f2545a8b62a6b0fc2ee060dc30559d\"> undistort </a>\n",
    "    The function transforms an image to compensate radial and tangential lens distortion. <br>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = cv.undistort(img, mtx, dist, None, newcameramtx)\n",
    "undistored = cv.undistort(img, mtx, dist, None, None)\n",
    "x,y,w,h = roi   \n",
    "dst = dst[y:y+h, x:x+w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprojection error: 10.099807080135967\n"
     ]
    }
   ],
   "source": [
    "mean_error = 0\n",
    "for i in range(len(obj_points)) :\n",
    "    reprojected_point,_ = cv.projectPoints(obj_points[i], rvecs[i], tvecs[i], mtx, dist)\n",
    "    error = cv.norm(img_points[i], reprojected_point, cv.NORM_L2)/len(reprojected_point)\n",
    "    mean_error += error\n",
    "\n",
    "print(\"Reprojection error: {}\".format(mean_error))\n",
    "\n",
    "while True :\n",
    "    cv.imshow('calibrated', dst)\n",
    "    cv.imshow('undistort', undistored)\n",
    "    if cv.waitKey(1) & 0xFF == ord('q') : \n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Saving parameters into numpy format\n",
    "# np.save(\"./new-camera_params/ret\", ret)\n",
    "# np.save(\"./new-camera_params/K\", mtx)\n",
    "# np.save(\"./new-camera_params/dist\", dist)\n",
    "# np.save(\"./new-camera_params/rvecs\", rvecs)\n",
    "# np.save(\"./new-camera_params/tvecs\", tvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_image(frame,im_src, pts_src, pts_dst):\n",
    "    \n",
    "    # Calculate Homography\n",
    "    h, status = cv.findHomography(pts_src, pts_dst)\n",
    "\n",
    "    # Warp source image to destination based on homography\n",
    "    warped_image = cv.warpPerspective(im_src, h, (frame.shape[1],frame.shape[0]))\n",
    "            \n",
    "    # Prepare a mask representing region to copy from the warped image into the original frame.\n",
    "    mask = np.zeros([frame.shape[0], frame.shape[1]], dtype=np.uint8)\n",
    "    cv.fillConvexPoly(mask, np.int32(pts_dst), (255, 255, 255), cv.LINE_AA)\n",
    "    im_out = cv.add(frame, warped_image, mask=cv.bitwise_not(mask))\n",
    "    im_out = cv.add(im_out, warped_image)\n",
    "    \n",
    "    return im_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create()\n",
    "bf = cv.BFMatcher() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Term1_65\\Image_Processing\\Image_Processing_github\\Final_Exam/new-camera_params/\n",
      "Camera matrix\n",
      "[[726.47630219   0.         466.25758541]\n",
      " [  0.         732.0640605  251.997867  ]\n",
      " [  0.           0.           1.        ]]\n",
      "Len distortion\n",
      "[[ 0.02450982  0.03012135  0.00045661  0.01036187 -0.07354287]]\n"
     ]
    }
   ],
   "source": [
    "params_dir = os.getcwd()+'/new-camera_params/' #Calibration -> Parallel image \n",
    "print(params_dir)\n",
    "\n",
    "# Load camera parameters\n",
    "# ret = np.load(params_dir+'ret.npy')\n",
    "K = np.load(params_dir+'K.npy')\n",
    "dist = np.load(params_dir+'dist.npy')\n",
    "# focal_length = (K[0,0] + K[1,1])/2\n",
    "\n",
    "print(\"Camera matrix\")\n",
    "print(K)\n",
    "print(\"Len distortion\")\n",
    "print(dist)\n",
    "\n",
    "# chessboard_size = (8,6) # Your chessboard size\n",
    "# # iterative termination criteria, maximum iterationm and epsilon\n",
    "# term_criteria = (cv.TermCriteria_EPS+ cv.TermCriteria_MAX_ITER, 30, 0.001)\n",
    "# # Defining the world coordinates for 3D points\n",
    "# objp = np.zeros((1, chessboard_size[0] * chessboard_size[1], 3), np.float32)\n",
    "# objp[0,:,:2] = np.mgrid[0:chessboard_size[0], 0:chessboard_size[1]].T.reshape(-1, 2)\n",
    "# # 3D object for visualization\n",
    "# axis = np.float32([[3,0,0], [0,3,0], [0,0,-3]]).reshape(-1,3)\n",
    "# box = np.float32([[0,0,0], [0,4,0], [4,4,0], [4,0,0],\n",
    "#                    [0,0,-4],[0,4,-4],[4,4,-4],[4,0,-4] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AruCo_dict = cv.aruco.Dictionary_get(cv.aruco.DICT_4X4_50) # Given Aruco\n",
    "# AruCo_params = cv.aruco.DetectorParameters_create()\n",
    "# board = cv.aruco.GridBoard_create(1, 1, 0.182, 0.0075, AruCo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_text(img, pose, dy, text) :\n",
    "#     x0 = pose[0]\n",
    "#     y0 = pose[1]\n",
    "#     for i, line in enumerate(text.split('\\n')) :\n",
    "#         y = y0 + i*dy\n",
    "#         cv.putText(img, line, np.int32([x0, y]), cv.FONT_HERSHEY_COMPLEX, 0.75, (50,200,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv.VideoCapture('Final_exam/Dataset-1/left_output-1.avi')\n",
    "template_img = cv.imread('Templates/Template-1.png') \n",
    "# qr = cv.imread('./QR-Code_new.jpg') \n",
    "# print(qr.shape)\n",
    "# im_src_size = qr.shape[:2] \n",
    "# src_points = np.float32([[-0.182,-0.240], [im_src_size[1],0],[im_src_size[1], im_src_size[0]] ,[0, im_src_size[0]] ]) \n",
    "template_gray = preprocessing(template_img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "while vid.isOpened() :\n",
    "    ret,  frame = vid.read()  \n",
    "\n",
    "    if ret :\n",
    "        \n",
    "        query_img = frame\n",
    "        kernel = np.array([[0,0,0], \n",
    "                           [0, 1.5,0],\n",
    "                           [0,0,0]])  \n",
    "        filter = cv.filter2D(query_img, -1, kernel) \n",
    "        query_gray = preprocessing(filter)\n",
    "    \n",
    "        template_kpts, template_desc = sift.detectAndCompute(template_img, None)\n",
    "        query_kpts, query_desc = sift.detectAndCompute(query_gray, None)\n",
    "        matches = bf.knnMatch(template_desc, query_desc, k=2)\n",
    "\n",
    "        good_matches = list()\n",
    "        good_matches_list = list()\n",
    "        for m, n in matches :\n",
    "            if m.distance < 0.7*n.distance :\n",
    "                good_matches.append(m)\n",
    "                good_matches_list.append([m])\n",
    "        \n",
    "\n",
    "        if len(good_matches) > 8 : \n",
    "            src_pts = np.float32([ template_kpts[m.queryIdx].pt for m in good_matches ]).reshape(-1,1,2)\n",
    "            dst_pts = np.float32([ query_kpts[m.trainIdx].pt for m in good_matches ]).reshape(-1,1,2)\n",
    "\n",
    "            H, inlier_masks = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 1.2) \n",
    "            \n",
    "            h, w = template_img.shape[:2]\n",
    "            template_box = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1,1,2) \n",
    "            transformed_box = cv.perspectiveTransform(template_box, H)\n",
    "\n",
    "            detected_img = cv.polylines( frame, [np.int32(transformed_box)], True, (0,0,255), 2, cv.LINE_AA)\n",
    "            drawmatch_img = cv.drawMatchesKnn(template_img, template_kpts, detected_img, query_kpts, good_matches_list, None, flags=2, matchesMask=inlier_masks)\n",
    "\n",
    "            # augmented = augmented_image(detected_img, qr, src_points,transformed_box)\n",
    "\n",
    "            # img = augmented.copy() \n",
    "            # markerCorners, markerIds, rejectedCandidates = cv.aruco.detectMarkers(augmented , AruCo_dict, parameters = AruCo_params)\n",
    "        \n",
    "            # if len(markerCorners) > 0:\n",
    "            #     img = cv.aruco.drawDetectedMarkers(augmented , markerCorners)\n",
    "            #     rvecs, tvecs, points = cv.aruco.estimatePoseSingleMarkers(markerCorners , 0.05, K, dist)\n",
    "            #     for (rvec, tvec, id, corner) in zip(rvecs, tvecs, markerIds, markerCorners) :\n",
    "            #         img = cv.drawFrameAxes(augmented , K, dist, rvec, tvec, 0.05)\n",
    "            #         x = tvec[0,0]\n",
    "            #         y = tvec[0,1]\n",
    "            #         z = tvec[0,2]\n",
    "            #         text = \" X :{:.3f} \\n Y :{:.3f} \\n Z :{:.3f}\".format( x, y, z) \n",
    "            #         cX = (corner[0,0][0] + corner[0,2][0])/ 2\n",
    "            #         cY = (corner[0,0][1] + corner[0,2][1])/ 2  \n",
    "            #         write_text(detected_img, (cX, cY), 30, text) \n",
    "            #     ret, brvec, btvec = cv.aruco.estimatePoseBoard(markerCorners, markerIds, board, K, dist, rvecs, tvecs)\n",
    "            #     if ret :\n",
    "            #        img = cv.drawFrameAxes(frame, K, dist, brvec, btvec, 0.05)\n",
    "\n",
    "\n",
    "            cv.imshow('Video exam',detected_img )\n",
    "           \n",
    "        if cv.waitKey(1) & 0xFF == ord('q') :\n",
    "            break\n",
    "    else :\n",
    "        break\n",
    "\n",
    "vid.release  ()\n",
    "cv.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid = cv.VideoCapture('Final_exam/Dataset-1/left_output-1.avi')\n",
    "# template_img = cv.imread('Templates/Template-4.png')\n",
    "# template_gray = preprocessing(template_img)\n",
    "\n",
    "# # plt.imshow(template_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sift = cv.SIFT_create()\n",
    "# bf = cv.BFMatcher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center\">VidoCapture is OpenVideo</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while vid.isOpened():\n",
    "    \n",
    "#     ret, frame = vid.read()\n",
    "    \n",
    "#     if ret :\n",
    "#         query_img = frame\n",
    "#         kernel = np.array([[0,0,0],\n",
    "#                            [0,1.5,0],\n",
    "#                            [0,0,0]])\n",
    "#         filter = cv.filter2D(query_img, -1,kernel)\n",
    "#         query_gray = preprocessing(filter)\n",
    "#         template_kpts, template_desc = sift.detectAndCompute(template_img, None)\n",
    "#         query_kpts, query_desc = sift.detectAndCompute(query_gray, None)\n",
    "#         matches = bf.knnMatch(template_desc, query_desc, k=2)\n",
    "    \n",
    "    \n",
    "#         good_matches = list()\n",
    "#         good_matches_list = list()\n",
    "#         for m, n in matches :\n",
    "#             if m.distance < 0.7*n.distance :\n",
    "#                 good_matches.append(m)\n",
    "#                 good_matches_list.append([m])\n",
    "    \n",
    "#         if len(good_matches) > 14 :\n",
    "#             src_pts = np.float32([ template_kpts[m.queryIdx].pt for m in good_matches ]).reshape(-1,1,2)\n",
    "#             dst_pts = np.float32([ query_kpts[m.trainIdx].pt for m in good_matches ]).reshape(-1,1,2)\n",
    "\n",
    "#             H, inlier_masks = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 10.0) # H RANSAC\n",
    "#             # get the bounding box around template image\n",
    "#             h, w = template_img.shape[:2]\n",
    "#             template_box = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1,1,2)\n",
    "#             transformed_box = cv.perspectiveTransform(template_box, H)\n",
    "\n",
    "#             detected_img = cv.polylines(frame, [np.int32(transformed_box)], True, (0,0,255), 3, cv.LINE_AA)\n",
    "#             drawmatch_img = cv.drawMatchesKnn(template_img, template_kpts, detected_img, query_kpts, good_matches_list, None, flags=2, matchesMask=inlier_masks)\n",
    "\n",
    "#         # detected, drawmatch =  feature_object_detection(template_img, query_gray, frame, 10)\n",
    "    \n",
    "#         cv.imshow('Video exam',detected_img)\n",
    "#         # if cv.waitKey(int(1000/20)) & 0xFF == ord('q') : # int(1000/20) ทำให้ video slowly\n",
    "#         if cv.waitKey(33) & 0xFF == ord('q'):\n",
    "#             break\n",
    "#     else :\n",
    "#             break\n",
    "\n",
    "# vid.release()\n",
    "# cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e593ac106456af50ce7af38f9671c411b49d6cd90f9b885e167f0f594e09038c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
